[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "imaging_ai",
    "section": "",
    "text": "Code\nimport sys\nsys.executable\n\n\n'/opt/hostedtoolcache/Python/3.10.16/x64/bin/python3'\ntitle: Task 1"
  },
  {
    "objectID": "index.html#data-handling-and-preprocessing-10-points",
    "href": "index.html#data-handling-and-preprocessing-10-points",
    "title": "imaging_ai",
    "section": "Data Handling and Preprocessing (10 Points)",
    "text": "Data Handling and Preprocessing (10 Points)\n\nYou can focus for now on loading the T1-weighted images and the matching labels.\n\nWe used the LoadImaged transform from MONAI to load multiple modalities (t1, t1ce, t2, flair) and corresponding labels. The images are concatenated into a single tensor using the ConcatItemsd transform, and the labels are remapped using RemapLabels to ensure the labels are correctly handled.\n\nCreate a dataloader for the data using PyTorch’s Dataloader (or Monai’s Dataloader class).\n\nThe dataloader is created using the get_dataloader function. This function: - Reads the split files (e.g., train.txt, validation.txt, test.txt) containing dataset paths. - Applies a transformation pipeline (get_transforms) that preprocesses and augments the data. - Uses MONAI’s CacheDataset for efficient loading and preprocessing. - Wraps the dataset in PyTorch’s DataLoader to iterate over batches during training and validation.\n\nCreate suitable augmentations for the task to solve. Please note: If you apply transformations to the input data, you should think about if you need to apply any transformation to the label of the image as well.\n\nAugmentations are applied in get_transforms: - RandFlipd flips the images and labels along different axes. - RandSpatialCropd crops the images and labels to the specified roi_size. The transformations account for labels by applying the same operations to both images and labels.\nthe code applies several augmentations in the get_transforms function: - Spatial augmentations: - RandFlipd flips the image and label along all three spatial axes with a probability of 0.5. - RandSpatialCropd crops both the images and labels to a predefined roi_size. - Normalization: - NormalizeIntensityd normalizes the image intensities channel-wise."
  },
  {
    "objectID": "index.html#data-splitting-5-points",
    "href": "index.html#data-splitting-5-points",
    "title": "imaging_ai",
    "section": "2. Data Splitting (5 Points)",
    "text": "2. Data Splitting (5 Points)\n\nThink about an appropriate split of the dataset into train, validation and test split. Briefly explain why it is good scientific practice to have separate Training, Validation and Test dataset?\n\nSeparating these datasets prevents overfitting and ensures that: - Training dataset is used for learning. - Validation dataset is used for tuning hyperparameters and evaluating performance during training. - Test dataset is used for final unbiased evaluation of the model’s performance.\n\nPlease define the patient ids for each split in a separate file, like test.txt or validation.yml\n\nPatient IDs are loaded from JSON files (train.txt, validation.txt, test.txt) in the split_dir. This ensures reproducibility and consistency in data splits."
  },
  {
    "objectID": "index.html#model-selection-and-training-20-points",
    "href": "index.html#model-selection-and-training-20-points",
    "title": "imaging_ai",
    "section": "3. Model Selection and Training (20 Points)",
    "text": "3. Model Selection and Training (20 Points)\n\nImplement a training pipeline to train the U-Net model of MONAI (or your own implementation if you want). Make use of the dataloader you created.\n\nThe training pipeline is implemented in the train() function: - The MONAI UNet is initialized with: - spatial_dims=3 (to handle 3D data). - The number of input channels matches the modalities (t1, t1ce, t2, flair), and the number of output channels corresponds to the number of segmentation classes. - Customizable encoder-decoder channels and strides. - The pipeline uses the dataloaders (train_loader and val_loader) generated from the get_dataloaders() function, which provides training and validation datasets with proper transformations and augmentations. - The optimizer is Adam, and the loss function is defined as the DiceLoss.\n\nAt the end of each epoch, you should run a validation of the models performance.\n\nYes, the pipeline includes a validation step at the end of each epoch: - The validation data is processed through the model using sliding_window_inference to handle 3D volumes. - The Dice metric is calculated using the MONAI DiceMetric class to assess the overlap between predictions and ground truth. - Validation metrics, including the Dice score for each class and the mean Dice score, are logged for monitoring performance.\n\nAs we do not want to waste unnecessary energie and time, please implement an early stopping condition, that stops the training when the performance of the model does not improve for a few epochs on the validation set.\n\nYes, early stopping is implemented: - The train() function monitors the mean Dice score on the validation set. - If the Dice score does not improve for a defined number of epochs (default: 10, controlled by config.early_stop_limit), training is stopped early to save time and computational resources.\n\nWhich loss function do you think suitable to train your model for this segmentation task? The DiceLoss is used for training, as it is well-suited for segmentation tasks. This loss directly optimizes the Dice score, which is a common metric for medical image segmentation. It ensures accurate overlap between the predicted and ground truth masks.\nPlease save the weights of the model that you trained. You can either save the weights after each epoch seperatly or you can override the previously saved model with the new best performing one after each validation.\n\nThe model weights are saved during training: - After each epoch, if the validation mean Dice score improves, the current model’s weights are saved as best_model.pth in the specified config.save_path directory. - Only the best-performing model is saved, ensuring the most optimal model is retained without using unnecessary storage for suboptimal models."
  },
  {
    "objectID": "index.html#testing-and-performance-evaluation-15-points",
    "href": "index.html#testing-and-performance-evaluation-15-points",
    "title": "imaging_ai",
    "section": "4. Testing and Performance Evaluation (15 Points)",
    "text": "4. Testing and Performance Evaluation (15 Points)\n\nThink about a suitable metric to evaluate the performance of your model on the test set. You can also use more than one metric to capture different aspects of the models performance. The suitable metric for evaluating the model’s performance on the test set is the Dice score, as it measures the overlap between predicted segmentation masks and ground truth labels. This metric is commonly used in medical image segmentation tasks.\nBriefly explain, which model should you evaluate? You get a trained model after each epoch? Should you evaluate all of them and pick the best performing one?\n\n\nThe best-performing model is saved during training (best_model.pth) based on the highest mean Dice score on the validation set.\nThis saved model should be evaluated on the test set, as it represents the model with the best generalization during training and validation.\nEvaluating all models from every epoch would be unnecessary and computationally expensive, as the best model has already been identified using the validation set.\n\n\nCode a test pipeline to evaluate your model’s performance."
  },
  {
    "objectID": "index.html#extension-to-new-modalities-5-bonus-points",
    "href": "index.html#extension-to-new-modalities-5-bonus-points",
    "title": "imaging_ai",
    "section": "5. Extension to New Modalities (5 Bonus Points)",
    "text": "5. Extension to New Modalities (5 Bonus Points)\n\nExtent the model to include multiple modalities of your choice as input (T2, T1ce, …). Justify your choice. The modalities are combined into a single tensor using ConcatItemsd in the get_transforms function.\n\n\nDifferent MRI modalities capture complementary information about the tumor:\n\nT1 and T1ce highlight the tumor core and enhancing regions.\nT2 and FLAIR are useful for visualizing edema and non-enhancing tumor areas.\n\nCombining modalities allows the model to leverage richer information, leading to improved segmentation performance and better differentiation between tumor sub-regions.\n\n\nTrain the model with the same data split and asses the performance of the model using your test pipeline. The training pipeline is designed to support multiple modalities as input:\n\nThe dataloader loads all modalities (t1, t1ce, t2, flair), and the concatenated tensor is passed to the UNet model for training. Using the same data split, the performance of the model can be assessed by running the test_model() function on the test set after training. The test pipeline evaluates the model’s performance using metrics such as the Dice score.\n\nWhy and how does the performance change now that you use multiple modalities compared to using a single T1-weighted image as input?\n\nEach modality highlights different tumor characteristics. For example: - T1ce captures enhancing regions. - T2 captures edema and fluid buildup. - FLAIR is effective for visualizing non-enhancing tumor areas. - Using multiple modalities provides the model with diverse and complementary information, enabling it to make more accurate segmentation predictions."
  },
  {
    "objectID": "reports/task1.html",
    "href": "reports/task1.html",
    "title": "Task 1",
    "section": "",
    "text": "Task 1\nThe Fourier transformation \\(f(x, y) \\rightarrowtail F(u, v)\\) of a greyscale image \\(f(x,y)\\) results in a band-limited signal in the spatial frequency range with maximum frequencies \\(f_{umax}\\) and \\(ƒ_{vmax}\\). For representation in the computer, the (partial) image is sampled in x direction with 20 sampling points per mm and in y direction with 10 sampling points per mm.\n\n\nWhat is the theoretical maximum value of \\(f_{umax}\\) and \\(f_{vmax}\\) if error-free image reconstruction from the digital image should be possible (not using any compressive-sensing techniques)? (6pts)\n\n\n\nAccording to the Nyquist sampling theorem, the maximum representable frequency (Nyquist frequency) in each direction is half the sampling frequency. The sampling frequency can be derived from the given sampling points per mm.\n\nSampling frequency in x is \\(f_{sx}\\) and the Nyquist frequency in x is \\(f_{umax}\\): \\[\\begin{align*}\n  &f_{sx} = 20\\;\\text{points/mm} = 20 \\times 10^3 \\, \\text{points/m} \\\\\n  \\implies &f_{umax} = \\frac{f_{sx}}{2} = 10.0 \\, \\text{cycles/mm}\n\\end{align*}\\]\nSampling frequency in y is \\(f_{sy}\\) and the Nyquist frequency in y is \\(f_{vmax}\\): \\[\\begin{align*}\n  &f_{sy} = 10 \\, \\text{points/mm} = 10 \\times 10^3 \\, \\text{points/m} \\\\\n  \\implies &f_{vmax} = \\frac{f_{sy}}{2} = 5.0 \\, \\text{cycles/mm}\n\\end{align*}\\]\n\nThis ensures error-free reconstruction, as the digital image will contain all frequency components of the original image within the Nyquist limit. Frequencies above these limits would result in aliasing, violating error-free reconstruction conditions.\n\n\nWhat is the minimum memory requirement for the color image \\(f_F(x, y)\\) when stored in a conventional computer system, if \\(1024\\) values are to be distinguished per color channel. Describe the image format to be used.\n\n\nTo start lets find the number of ixels\nLet the image dimensions in mm be \\(L_x\\) (width) and \\(L_y\\) (height).\n- Pixels in \\(x\\)-direction: \\(N_x = 10.0 \\cdot L_x\\) - Pixels in \\(y\\)-direction: \\(N_y = 5.0 \\cdot L_y\\) - Total number of pixels: \\[\nN_{\\text{pixels}} = N_x \\cdot N_y = 50.0 \\cdot L_x \\cdot L_y\n\\]\nEach pixel in a color image has values for three color channels: Red, Green, and Blue (RGB). Each channel can store \\(1024\\) distinct values, which means \\(log_2^{1024} = 10.0\\) bits per channel.\nTotal bits per pixel: \\(b = 10.0 \\times 3 = 30.0\\) bits/pixel.\nThe memory requirement is the product of the number of pixels and bits per pixel: \\[\n\\text{Used Memory} = N_{\\text{pixels}} \\cdot b = (50.0 \\cdot L_x \\cdot L_y) \\cdot b \\, \\text{bits} = 6.25 \\cdot L_x \\cdot L_y \\cdot 30.0 \\, \\text{bytes} = 187.5 \\cdot L_x \\cdot L_y \\, \\text{bytes}\n\\]\n\n\nHow many colors could be represented with the quantization chosen in sub-task 3? (2pts)\n\n\nEach channel (Red, Green, and Blue) can represent 1024 intensity levels. With 10 bits per channel and 3 channels, the total number of colors is: \\[\n\\text{Total colors} = 1024^3 = 1,073,741,824\n\\]"
  },
  {
    "objectID": "reports/task2.html",
    "href": "reports/task2.html",
    "title": "task 2",
    "section": "",
    "text": "Task 2\nFor the subjective enhancement of a greyscale image \\(G = g(x, y)\\) , a transformation \\(T_G\\) is performed as a so-called gamma correction in the form \\(T_G : g \\rightarrow f\\) with \\(f(x, y) = c g^\\gamma (x, y)\\) where \\(g, f \\in [0, 255]\\).\n\nSketch the transformation curve \\(T_G\\) for \\(\\gamma_1 = 0.5\\) and \\(\\gamma_2 = 2\\)\n\n\nThe first step is to find the values of \\(c\\) for both cases. Since \\(\\max(f) = \\max(g) = 255\\), we have \\(c = 255 / 255^\\gamma\\).\n\n\nCode\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ndef draw_transform_curve(gamma: float, ax: plt.Axes = None, label: bool = True):\n    if not ax:\n        fig, ax = plt.subplots()\n    x = np.linspace(0, 255, 256)\n    c = 255 / 255**gamma\n    y = c * x**gamma\n    message = f\"$f = {c:0.4f} \\\\times g^{{{gamma}}}$\"\n    if label:\n        ax.plot(x, y, label=message)\n    else:\n        ax.plot(x, y)\n    ax.set_xlabel(\"g\")\n    if label:\n        ax.set_ylabel(\"f\")\n    else:\n        ax.set_ylabel(message)\n\n\nfig, ax = plt.subplots()\nfor gamma in [0.5, 2]:\n    draw_transform_curve(gamma, ax)\nax.set_title(f\"Transformation curve for $\\\\gamma=0.5$ and $\\\\gamma=2$\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nHow is the coeficient c typically determined? (2pts)\n\n\nThe coefficient \\(c\\) is typically determined such that the maximum value of the input image is mapped to the maximum value of the output image. This is done to ensure that the full dynamic range of the output image is used.\nAs mentioned above, \\(c = 255 / 255^\\gamma\\).\n\n\nIn which respect and for which type of input images \\(G\\) do the two gamma values \\(\\gamma_1,\\;\\gamma_2\\) lead to an image enhancement respectively? (2pts)\n\n\nFor \\(\\gamma &lt; 1\\), the transformation curve is concave, which means that the lower intensity values are stretched more than the higher intensity values. This leads to a brighter image with more contrast. This is useful for images with low contrast.\nFor \\(\\gamma &gt; 1\\), the transformation curve is convex, which means that the higher intensity values are stretched more than the lower intensity values. This leads to a darker image with more contrast. This is useful for images with high contrast.\n\n\nWhat should be the minimum slope of the transform function?\n\nfor a grey value spread (2pts)\nfor a grey value compression (2pts)\n\n\n\nIt’s important to note that a slope of exactly 1 implies no change in contrast, as the transformation function becomes an identity mapping. Also, a slope of 0 implies that the output image will be a constant value, which is not useful for image enhancement.\n\nFor a grey value spread, the minimum slope of the transform function should be 1.\nFor a grey value compression, the minimum slope of the transform function should be 0 (and smaller than 1). For instance, in this function:\n\n\n\n\n\n\n\n\n\n\nAs we can see, the gray values between (80, 160) are streched between (40, 200) which has a slope greater than 1. On the other hand, the gray values between (160, 255) are compressed between (200, 255) which has a slope smaller than 1."
  },
  {
    "objectID": "reports/task4.html",
    "href": "reports/task4.html",
    "title": "Task 4",
    "section": "",
    "text": "You can focus for now on loading the T1-weighted images and the matching labels.\n\nWe used the LoadImaged transform from MONAI to load multiple modalities (t1, t1ce, t2, flair) and corresponding labels. The images are concatenated into a single tensor using the ConcatItemsd transform, and the labels are remapped using RemapLabels to ensure the labels are correctly handled.\n\nCreate a dataloader for the data using PyTorch’s Dataloader (or Monai’s Dataloader class).\n\nThe dataloader is created using the get_dataloader function. This function: - Reads the split files (e.g., train.txt, validation.txt, test.txt) containing dataset paths. - Applies a transformation pipeline (get_transforms) that preprocesses and augments the data. - Uses MONAI’s CacheDataset for efficient loading and preprocessing. - Wraps the dataset in PyTorch’s DataLoader to iterate over batches during training and validation.\n\nCreate suitable augmentations for the task to solve. Please note: If you apply transformations to the input data, you should think about if you need to apply any transformation to the label of the image as well.\n\nAugmentations are applied in get_transforms: - RandFlipd flips the images and labels along different axes. - RandSpatialCropd crops the images and labels to the specified roi_size. The transformations account for labels by applying the same operations to both images and labels.\nthe code applies several augmentations in the get_transforms function: - Spatial augmentations: - RandFlipd flips the image and label along all three spatial axes with a probability of 0.5. - RandSpatialCropd crops both the images and labels to a predefined roi_size. - Normalization: - NormalizeIntensityd normalizes the image intensities channel-wise.\n\n\n\n\nThink about an appropriate split of the dataset into train, validation and test split. Briefly explain why it is good scientific practice to have separate Training, Validation and Test dataset?\n\nSeparating these datasets prevents overfitting and ensures that: - Training dataset is used for learning. - Validation dataset is used for tuning hyperparameters and evaluating performance during training. - Test dataset is used for final unbiased evaluation of the model’s performance.\n\nPlease define the patient ids for each split in a separate file, like test.txt or validation.yml\n\nPatient IDs are loaded from JSON files (train.txt, validation.txt, test.txt) in the split_dir. This ensures reproducibility and consistency in data splits.\n\n\n\n\nImplement a training pipeline to train the U-Net model of MONAI (or your own implementation if you want). Make use of the dataloader you created.\n\nThe training pipeline is implemented in the train() function: - The MONAI UNet is initialized with: - spatial_dims=3 (to handle 3D data). - The number of input channels matches the modalities (t1, t1ce, t2, flair), and the number of output channels corresponds to the number of segmentation classes. - Customizable encoder-decoder channels and strides. - The pipeline uses the dataloaders (train_loader and val_loader) generated from the get_dataloaders() function, which provides training and validation datasets with proper transformations and augmentations. - The optimizer is Adam, and the loss function is defined as the DiceLoss.\n\nAt the end of each epoch, you should run a validation of the models performance.\n\nYes, the pipeline includes a validation step at the end of each epoch: - The validation data is processed through the model using sliding_window_inference to handle 3D volumes. - The Dice metric is calculated using the MONAI DiceMetric class to assess the overlap between predictions and ground truth. - Validation metrics, including the Dice score for each class and the mean Dice score, are logged for monitoring performance.\n\nAs we do not want to waste unnecessary energie and time, please implement an early stopping condition, that stops the training when the performance of the model does not improve for a few epochs on the validation set.\n\nYes, early stopping is implemented: - The train() function monitors the mean Dice score on the validation set. - If the Dice score does not improve for a defined number of epochs (default: 10, controlled by config.early_stop_limit), training is stopped early to save time and computational resources.\n\nWhich loss function do you think suitable to train your model for this segmentation task? The DiceLoss is used for training, as it is well-suited for segmentation tasks. This loss directly optimizes the Dice score, which is a common metric for medical image segmentation. It ensures accurate overlap between the predicted and ground truth masks.\nPlease save the weights of the model that you trained. You can either save the weights after each epoch seperatly or you can override the previously saved model with the new best performing one after each validation.\n\nThe model weights are saved during training: - After each epoch, if the validation mean Dice score improves, the current model’s weights are saved as best_model.pth in the specified config.save_path directory. - Only the best-performing model is saved, ensuring the most optimal model is retained without using unnecessary storage for suboptimal models.\n\n\n\n\nThink about a suitable metric to evaluate the performance of your model on the test set. You can also use more than one metric to capture different aspects of the models performance. The suitable metric for evaluating the model’s performance on the test set is the Dice score, as it measures the overlap between predicted segmentation masks and ground truth labels. This metric is commonly used in medical image segmentation tasks.\nBriefly explain, which model should you evaluate? You get a trained model after each epoch? Should you evaluate all of them and pick the best performing one?\n\n\nThe best-performing model is saved during training (best_model.pth) based on the highest mean Dice score on the validation set.\nThis saved model should be evaluated on the test set, as it represents the model with the best generalization during training and validation.\nEvaluating all models from every epoch would be unnecessary and computationally expensive, as the best model has already been identified using the validation set.\n\n\nCode a test pipeline to evaluate your model’s performance.\n\n\n\n\n\nExtent the model to include multiple modalities of your choice as input (T2, T1ce, …). Justify your choice. The modalities are combined into a single tensor using ConcatItemsd in the get_transforms function.\n\n\nDifferent MRI modalities capture complementary information about the tumor:\n\nT1 and T1ce highlight the tumor core and enhancing regions.\nT2 and FLAIR are useful for visualizing edema and non-enhancing tumor areas.\n\nCombining modalities allows the model to leverage richer information, leading to improved segmentation performance and better differentiation between tumor sub-regions.\n\n\nTrain the model with the same data split and asses the performance of the model using your test pipeline. The training pipeline is designed to support multiple modalities as input:\n\nThe dataloader loads all modalities (t1, t1ce, t2, flair), and the concatenated tensor is passed to the UNet model for training. Using the same data split, the performance of the model can be assessed by running the test_model() function on the test set after training. The test pipeline evaluates the model’s performance using metrics such as the Dice score.\n\nWhy and how does the performance change now that you use multiple modalities compared to using a single T1-weighted image as input?\n\nEach modality highlights different tumor characteristics. For example: - T1ce captures enhancing regions. - T2 captures edema and fluid buildup. - FLAIR is effective for visualizing non-enhancing tumor areas. - Using multiple modalities provides the model with diverse and complementary information, enabling it to make more accurate segmentation predictions."
  },
  {
    "objectID": "reports/task4.html#data-handling-and-preprocessing-10-points",
    "href": "reports/task4.html#data-handling-and-preprocessing-10-points",
    "title": "Task 4",
    "section": "",
    "text": "You can focus for now on loading the T1-weighted images and the matching labels.\n\nWe used the LoadImaged transform from MONAI to load multiple modalities (t1, t1ce, t2, flair) and corresponding labels. The images are concatenated into a single tensor using the ConcatItemsd transform, and the labels are remapped using RemapLabels to ensure the labels are correctly handled.\n\nCreate a dataloader for the data using PyTorch’s Dataloader (or Monai’s Dataloader class).\n\nThe dataloader is created using the get_dataloader function. This function: - Reads the split files (e.g., train.txt, validation.txt, test.txt) containing dataset paths. - Applies a transformation pipeline (get_transforms) that preprocesses and augments the data. - Uses MONAI’s CacheDataset for efficient loading and preprocessing. - Wraps the dataset in PyTorch’s DataLoader to iterate over batches during training and validation.\n\nCreate suitable augmentations for the task to solve. Please note: If you apply transformations to the input data, you should think about if you need to apply any transformation to the label of the image as well.\n\nAugmentations are applied in get_transforms: - RandFlipd flips the images and labels along different axes. - RandSpatialCropd crops the images and labels to the specified roi_size. The transformations account for labels by applying the same operations to both images and labels.\nthe code applies several augmentations in the get_transforms function: - Spatial augmentations: - RandFlipd flips the image and label along all three spatial axes with a probability of 0.5. - RandSpatialCropd crops both the images and labels to a predefined roi_size. - Normalization: - NormalizeIntensityd normalizes the image intensities channel-wise."
  },
  {
    "objectID": "reports/task4.html#data-splitting-5-points",
    "href": "reports/task4.html#data-splitting-5-points",
    "title": "Task 4",
    "section": "",
    "text": "Think about an appropriate split of the dataset into train, validation and test split. Briefly explain why it is good scientific practice to have separate Training, Validation and Test dataset?\n\nSeparating these datasets prevents overfitting and ensures that: - Training dataset is used for learning. - Validation dataset is used for tuning hyperparameters and evaluating performance during training. - Test dataset is used for final unbiased evaluation of the model’s performance.\n\nPlease define the patient ids for each split in a separate file, like test.txt or validation.yml\n\nPatient IDs are loaded from JSON files (train.txt, validation.txt, test.txt) in the split_dir. This ensures reproducibility and consistency in data splits."
  },
  {
    "objectID": "reports/task4.html#model-selection-and-training-20-points",
    "href": "reports/task4.html#model-selection-and-training-20-points",
    "title": "Task 4",
    "section": "",
    "text": "Implement a training pipeline to train the U-Net model of MONAI (or your own implementation if you want). Make use of the dataloader you created.\n\nThe training pipeline is implemented in the train() function: - The MONAI UNet is initialized with: - spatial_dims=3 (to handle 3D data). - The number of input channels matches the modalities (t1, t1ce, t2, flair), and the number of output channels corresponds to the number of segmentation classes. - Customizable encoder-decoder channels and strides. - The pipeline uses the dataloaders (train_loader and val_loader) generated from the get_dataloaders() function, which provides training and validation datasets with proper transformations and augmentations. - The optimizer is Adam, and the loss function is defined as the DiceLoss.\n\nAt the end of each epoch, you should run a validation of the models performance.\n\nYes, the pipeline includes a validation step at the end of each epoch: - The validation data is processed through the model using sliding_window_inference to handle 3D volumes. - The Dice metric is calculated using the MONAI DiceMetric class to assess the overlap between predictions and ground truth. - Validation metrics, including the Dice score for each class and the mean Dice score, are logged for monitoring performance.\n\nAs we do not want to waste unnecessary energie and time, please implement an early stopping condition, that stops the training when the performance of the model does not improve for a few epochs on the validation set.\n\nYes, early stopping is implemented: - The train() function monitors the mean Dice score on the validation set. - If the Dice score does not improve for a defined number of epochs (default: 10, controlled by config.early_stop_limit), training is stopped early to save time and computational resources.\n\nWhich loss function do you think suitable to train your model for this segmentation task? The DiceLoss is used for training, as it is well-suited for segmentation tasks. This loss directly optimizes the Dice score, which is a common metric for medical image segmentation. It ensures accurate overlap between the predicted and ground truth masks.\nPlease save the weights of the model that you trained. You can either save the weights after each epoch seperatly or you can override the previously saved model with the new best performing one after each validation.\n\nThe model weights are saved during training: - After each epoch, if the validation mean Dice score improves, the current model’s weights are saved as best_model.pth in the specified config.save_path directory. - Only the best-performing model is saved, ensuring the most optimal model is retained without using unnecessary storage for suboptimal models."
  },
  {
    "objectID": "reports/task4.html#testing-and-performance-evaluation-15-points",
    "href": "reports/task4.html#testing-and-performance-evaluation-15-points",
    "title": "Task 4",
    "section": "",
    "text": "Think about a suitable metric to evaluate the performance of your model on the test set. You can also use more than one metric to capture different aspects of the models performance. The suitable metric for evaluating the model’s performance on the test set is the Dice score, as it measures the overlap between predicted segmentation masks and ground truth labels. This metric is commonly used in medical image segmentation tasks.\nBriefly explain, which model should you evaluate? You get a trained model after each epoch? Should you evaluate all of them and pick the best performing one?\n\n\nThe best-performing model is saved during training (best_model.pth) based on the highest mean Dice score on the validation set.\nThis saved model should be evaluated on the test set, as it represents the model with the best generalization during training and validation.\nEvaluating all models from every epoch would be unnecessary and computationally expensive, as the best model has already been identified using the validation set.\n\n\nCode a test pipeline to evaluate your model’s performance."
  },
  {
    "objectID": "reports/task4.html#extension-to-new-modalities-5-bonus-points",
    "href": "reports/task4.html#extension-to-new-modalities-5-bonus-points",
    "title": "Task 4",
    "section": "",
    "text": "Extent the model to include multiple modalities of your choice as input (T2, T1ce, …). Justify your choice. The modalities are combined into a single tensor using ConcatItemsd in the get_transforms function.\n\n\nDifferent MRI modalities capture complementary information about the tumor:\n\nT1 and T1ce highlight the tumor core and enhancing regions.\nT2 and FLAIR are useful for visualizing edema and non-enhancing tumor areas.\n\nCombining modalities allows the model to leverage richer information, leading to improved segmentation performance and better differentiation between tumor sub-regions.\n\n\nTrain the model with the same data split and asses the performance of the model using your test pipeline. The training pipeline is designed to support multiple modalities as input:\n\nThe dataloader loads all modalities (t1, t1ce, t2, flair), and the concatenated tensor is passed to the UNet model for training. Using the same data split, the performance of the model can be assessed by running the test_model() function on the test set after training. The test pipeline evaluates the model’s performance using metrics such as the Dice score.\n\nWhy and how does the performance change now that you use multiple modalities compared to using a single T1-weighted image as input?\n\nEach modality highlights different tumor characteristics. For example: - T1ce captures enhancing regions. - T2 captures edema and fluid buildup. - FLAIR is effective for visualizing non-enhancing tumor areas. - Using multiple modalities provides the model with diverse and complementary information, enabling it to make more accurate segmentation predictions."
  },
  {
    "objectID": "reports/task3.html",
    "href": "reports/task3.html",
    "title": "task 3",
    "section": "",
    "text": "Task 3\nIn this task you will need to perform threshold-based image analysis:\n\nRead the greyscale image brain.png, which is provided on the lecture homepage. Reduce the salt and pepper noise in the image using a median filter. (3pts)\n\n\n\n\nCode\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n\nimg_noise = cv2.imread(\"brain-noisy.png\", cv2.IMREAD_GRAYSCALE)\nif img_noise is None:\n    img_noise = cv2.imread(\"./reports/brain-noisy.png\", cv2.IMREAD_GRAYSCALE)\nassert img_noise is not None, \"Image not found {}\".format(os.listdir())\nimg = cv2.medianBlur(img_noise, 5)\nimg = cv2.GaussianBlur(img, (5, 5), 0)\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nax[0].imshow(img_noise, cmap=\"gray\")\nax[0].set_title(\"Original image\")\nax[0].axis(\"off\")\nax[1].imshow(cv2.medianBlur(img_noise, 3), cmap=\"gray\")\nax[1].set_title(\"Median filtered image (3x3)\")\nax[1].axis(\"off\")\nax[2].imshow(img, cmap=\"gray\")\nax[2].set_title(\"Median filtered image (5x5)\")\nax[2].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see the kernel size of \\(3 \\times 3\\) is not enough to remove the noise, while the kernel size of \\(5 \\times 5\\) is sufficient.\n\n\nOtsu thresholding is a histogram-based method for image segmentation. Use it to find an intensity threshold to segment brain pixels from background. Use Otsu thresholding again to find the threshold only over the brain pixels to segment brain’s grey matter from the white matter. Using the two thresholds create three binary masks brain-bg.png, brain-gm.png, brain-wm.png, which should be white in regions of background, grey matter, and white matter, respectively, and black elsewhere. (4pts)\n\n\n\n\nCode\nvalues, bin_edge = np.histogram(img, bins=256, range=(0, 256))\nbin_centers = (bin_edge[:-1] + bin_edge[1:]) / 2\n# values = values[1:]\n# bin_centers = bin_centers[1:]\nm = values.mean() * 2\nvalues[values &gt; m] = m\n\nplt.bar(bin_centers, values, lw=2)\nplt.title(\"Bounded histogram of the image (values capped at 2x the mean)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe correct way to use Otsu thresholding with several values is to use [@arora2008multilevel], which is not implemented in OpenCV. However, we can use the implementation in the skimage library (which implemented based on [@liao2001fast])\n\n\nCode\nfrom skimage.filters import threshold_multiotsu\n\n\ndef otsu_threshold(\n    img: np.ndarray, classes: int\n) -&gt; tuple[list[np.ndarray], np.ndarray]:\n    threshold = threshold_multiotsu(img, classes=classes).tolist()\n    threshold = [0] + threshold + [255]\n    assert (\n        len(threshold) == classes + 1\n    ), \"The number of thresholds should be equal to the number of classes - 1\"\n    masks = [(img &gt;= t1) & (img &lt; t2) for t1, t2 in zip(threshold, threshold[1:])]\n    # masks.append(img &gt;= threshold[-1])\n    assert all(mask.dtype == bool for mask in masks), \"Masks should be boolean\"\n    assert (\n        len(masks) == classes\n    ), \"The number of masks should be equal to the number of classes\"\n    return masks, threshold[1:-1]\n\n\n(brain_bg, brain_gm, brain_wm), threshold = otsu_threshold(img, 3)\n\n\n\n\nCode\ncolors = [\"r\", \"g\", \"y\"]\n(brain_bg, brain_gm, brain_wm), threshold = otsu_threshold(img, 3)\n\nprint(f\"Threshold for the whole image: {threshold}\")\n\nvalues, bin_edge = np.histogram(img, bins=256, range=(0, 256))\nbin_centers = (bin_edge[:-1] + bin_edge[1:]) / 2\nm = values.mean() * 2\nvalues[values &gt; m] = m\n\nplt.bar(bin_centers, values, lw=2)\nfor th, color in zip(threshold, colors):\n    plt.axvline(th, color=color, lw=2, ls=\"--\", label=f\"Threshold: {th}\")\nplt.legend()\nplt.title(\"Bounded histogram of the image (values capped at 2x the mean)\")\nplt.show()\n\n\nThreshold for the whole image: [77, 182]\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(1, 4, figsize=(15, 5))\nax[0].imshow(brain_bg, cmap=\"gray\")\nax[0].set_title(\"Background\")\nax[0].axis(\"off\")\n\nax[1].imshow(brain_gm, cmap=\"gray\")\nax[1].set_title(\"Grey matter\")\nax[1].axis(\"off\")\n\nax[2].imshow(brain_wm, cmap=\"gray\")\nax[2].set_title(\"White matter\")\nax[2].axis(\"off\")\n\nax[3].imshow(brain_bg * 1 + brain_gm * 2 + brain_wm * 3, cmap=\"gray\")\nax[3].set_title(\"All\")\nax[3].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPlot a log-scaled histogram of the image, which should show how frequently different intensity values occur in the image. How could you roughly estimate the two thresholds you found in the previous task just by looking at the histogram? (3pts)\n\n\n\n\nCode\nvalues, bin_edge = np.histogram(img, bins=256, range=(0, 256))\nbin_centers = (bin_edge[:-1] + bin_edge[1:]) / 2\nplt.bar(bin_centers, values, lw=2)\nplt.yscale(\"log\")\n\nfor th, color in zip(threshold, colors):\n    plt.axvline(th, color=color, lw=2, ls=\"--\", label=f\"Threshold: {th}\")\n\nplt.legend()\nplt.title(\"Log-scaled histogram of the image\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see, the histogram has two peaks, which correspond to the grey matter and white matter. The two thresholds can be estimated by finding the two peaks in the histogram. (The purpose of otsu thresholding is to find the optimal threshold for the two peaks)\n\n\nCombine the three masks into a single colour image so that background, grey matter, and white matter are mapped to red, green and blue, respectively. (3pts)\n\n\n\n\nCode\ncombined_brain = np.stack([brain_bg, brain_gm, brain_wm], axis=-1).astype(np.uint8) * 255\n\nplt.imshow(combined_brain)\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUse erosion (or any other morphological) filter to produce a border between the grey and white matter. Overlay that border on the denoised input image. (3pts)\n\n\n\n\nCode\nkernel = np.ones((3, 3), np.uint8)\nbrain_wm_eroded = cv2.erode(brain_wm.astype(np.uint8), kernel, iterations=1)\nbrain_wm_dilated = cv2.dilate(brain_wm_eroded, kernel, iterations=1)\nborder = (brain_wm_dilated - brain_wm_eroded) * 255\nalpha = 0.85\nbordered_img = cv2.addWeighted(img, alpha, border, 1 - alpha, 0)\n\n\n# plt.imshow(img, cmap=\"gray\")\n# plt.imshow(border, cmap=\"gray\", alpha=0.5)\nplt.imshow(bordered_img, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUse bilinear interpolation to up-sample the image by a factor of four along each axis. Apply the same thresholds as in 2) to obtain a segmentation into background, grey matter, and white matter. Up-sample the masks from 2) in the same way and compare the up-sampled masks to the masks from the up-sampled image. Can you see a diference? Why? Repeat the same procedure using nearest neighbour interpolation. Can you see a diference now? (4pts)\n\n\n\n\nCode\ndef upsample(img: np.ndarray, factor: int, interpolation: int) -&gt; np.ndarray:\n    return cv2.resize(\n        img, (img.shape[1] * factor, img.shape[0] * factor), interpolation=interpolation\n    )\n\n\nmasks, threshold = otsu_threshold(img, 3)\nimg_upsampled = upsample(img, 4, cv2.INTER_LINEAR)\nmasks_upsampled, threshold_upsampled = otsu_threshold(img_upsampled, 3)\n\nfig, ax = plt.subplots(2, 4, figsize=(15, 10))\nfig.suptitle(\n    \"Comparison of upsampled masks and upsampled image using linear interpolation\",\n    fontsize=16,\n)\n\ntitles = [\"Background\", \"Grey matter\", \"White matter\", \"All\"]\nmasks.append(masks[0] * 1 + masks[1] * 2 + masks[2] * 3)\nmasks_upsampled.append(\n    masks_upsampled[0] * 1 + masks_upsampled[1] * 2 + masks_upsampled[2] * 3\n)\n\nfor i, (mask, mask_upsampled, title) in enumerate(zip(masks, masks_upsampled, titles)):\n    ax[0, i].imshow(mask, cmap=\"gray\")\n    ax[0, i].set_title(title)\n    ax[0, i].axis(\"off\")\n\n    ax[1, i].imshow(mask_upsampled, cmap=\"gray\")\n    ax[1, i].set_title(f\"{title} upsampled\")\n    ax[1, i].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nClearly, we can see much smoother edges in the upsampled masks compared to the upsampled image. This is because the interpolation method used in the up-sampling process is linear, which smooths the edges.\nNow, let’s repeat the same procedure using the nearest neighbour interpolation method.\n\n\nCode\nmasks, threshold = otsu_threshold(img, 3)\nimg_upsampled = upsample(img, 4, cv2.INTER_NEAREST)\nmasks_upsampled, threshold_upsampled = otsu_threshold(img_upsampled, 3)\n\nfig, ax = plt.subplots(2, 4, figsize=(15, 10))\nfig.suptitle(\n    \"Comparison of upsampled masks and upsampled image using nearest neighbour interpolation\",\n    fontsize=16,\n)\n\ntitles = [\"Background\", \"Grey matter\", \"White matter\", \"All\"]\nmasks.append(masks[0] * 1 + masks[1] * 2 + masks[2] * 3)\nmasks_upsampled.append(\n    masks_upsampled[0] * 1 + masks_upsampled[1] * 2 + masks_upsampled[2] * 3\n)\n\nfor i, (mask, mask_upsampled, title) in enumerate(zip(masks, masks_upsampled, titles)):\n    ax[0, i].imshow(mask, cmap=\"gray\")\n    ax[0, i].set_title(title)\n    ax[0, i].axis(\"off\")\n\n    ax[1, i].imshow(mask_upsampled, cmap=\"gray\")\n    ax[1, i].set_title(f\"{title} upsampled\")\n    ax[1, i].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see the edges are much sharper in the upsampled masks compared to the upsampled image. This is because the nearest neighbour interpolation method does not smooth the edges.\nTODO: Test same thing with pyrUp\n\n\nCode\ndef upsample_pyramid_(img: np.ndarray) -&gt; np.ndarray:\n    # return cv2.resize(\n    #     img, (img.shape[1] * factor, img.shape[0] * factor), interpolation=interpolation\n    # )\n    return cv2.pyrUp(img, dstsize=(img.shape[1] * 2, img.shape[0] * 2))\n\ndef upsample_pyramid(img: np.ndarray, factor: int) -&gt; np.ndarray:\n    if factor &lt;= 1:\n        raise ValueError(\"Factor should be greater than 1\")\n    f = 1\n    while f &lt; factor:\n        img = upsample_pyramid_(img)\n        f *= 2\n    return img\n\n\nprint(img.shape)\nimg_upsampled = upsample_pyramid(img, 4)\nprint(img_upsampled.shape)\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(img_upsampled, cmap=\"gray\")\nax[0].set_title(\"Upsampled image\")\nax[0].axis(\"off\")\n\nax[1].imshow(img, cmap=\"gray\")\nax[1].set_title(\"Original image\")\nax[1].axis(\"off\")\n\nplt.show()\n\n\n(145, 145)\n(580, 580)"
  }
]