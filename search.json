[
  {
    "objectID": "reports/task3.html",
    "href": "reports/task3.html",
    "title": "task 3",
    "section": "",
    "text": "Task 3\nIn this task you will need to perform threshold-based image analysis:\n\nRead the greyscale image brain.png, which is provided on the lecture homepage. Reduce the salt and pepper noise in the image using a median filter. (3pts)\n\n\n\n\nCode\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n\nimg_noise = cv2.imread(\"brain-noisy.png\", cv2.IMREAD_GRAYSCALE)\nif img_noise is None:\n    img_noise = cv2.imread(\"./reports/brain-noisy.png\", cv2.IMREAD_GRAYSCALE)\nassert img_noise is not None, \"Image not found {}\".format(os.listdir())\nimg = cv2.medianBlur(img_noise, 5)\nimg = cv2.GaussianBlur(img, (5, 5), 0)\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nax[0].imshow(img_noise, cmap=\"gray\")\nax[0].set_title(\"Original image\")\nax[0].axis(\"off\")\nax[1].imshow(cv2.medianBlur(img_noise, 3), cmap=\"gray\")\nax[1].set_title(\"Median filtered image (3x3)\")\nax[1].axis(\"off\")\nax[2].imshow(img, cmap=\"gray\")\nax[2].set_title(\"Median filtered image (5x5)\")\nax[2].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see the kernel size of \\(3 \\times 3\\) is not enough to remove the noise, while the kernel size of \\(5 \\times 5\\) is sufficient.\n\n\nOtsu thresholding is a histogram-based method for image segmentation. Use it to find an intensity threshold to segment brain pixels from background. Use Otsu thresholding again to find the threshold only over the brain pixels to segment brain’s grey matter from the white matter. Using the two thresholds create three binary masks brain-bg.png, brain-gm.png, brain-wm.png, which should be white in regions of background, grey matter, and white matter, respectively, and black elsewhere. (4pts)\n\n\n\n\nCode\nvalues, bin_edge = np.histogram(img, bins=256, range=(0, 256))\nbin_centers = (bin_edge[:-1] + bin_edge[1:]) / 2\n# values = values[1:]\n# bin_centers = bin_centers[1:]\nm = values.mean() * 2\nvalues[values &gt; m] = m\n\nplt.bar(bin_centers, values, lw=2)\nplt.title(\"Bounded histogram of the image (values capped at 2x the mean)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe correct way to use Otsu thresholding with several values is to use [@arora2008multilevel], which is not implemented in OpenCV. However, we can use the implementation in the skimage library (which implemented based on [@liao2001fast])\n\n\nCode\nfrom skimage.filters import threshold_multiotsu\n\n\ndef otsu_threshold(\n    img: np.ndarray, classes: int\n) -&gt; tuple[list[np.ndarray], np.ndarray]:\n    threshold = threshold_multiotsu(img, classes=classes).tolist()\n    threshold = [0] + threshold + [255]\n    assert (\n        len(threshold) == classes + 1\n    ), \"The number of thresholds should be equal to the number of classes - 1\"\n    masks = [(img &gt;= t1) & (img &lt; t2) for t1, t2 in zip(threshold, threshold[1:])]\n    # masks.append(img &gt;= threshold[-1])\n    assert all(mask.dtype == bool for mask in masks), \"Masks should be boolean\"\n    assert (\n        len(masks) == classes\n    ), \"The number of masks should be equal to the number of classes\"\n    return masks, threshold[1:-1]\n\n\n(brain_bg, brain_gm, brain_wm), threshold = otsu_threshold(img, 3)\n\n\n\n\nCode\ncolors = [\"r\", \"g\", \"y\"]\n(brain_bg, brain_gm, brain_wm), threshold = otsu_threshold(img, 3)\n\nprint(f\"Threshold for the whole image: {threshold}\")\n\nvalues, bin_edge = np.histogram(img, bins=256, range=(0, 256))\nbin_centers = (bin_edge[:-1] + bin_edge[1:]) / 2\nm = values.mean() * 2\nvalues[values &gt; m] = m\n\nplt.bar(bin_centers, values, lw=2)\nfor th, color in zip(threshold, colors):\n    plt.axvline(th, color=color, lw=2, ls=\"--\", label=f\"Threshold: {th}\")\nplt.legend()\nplt.title(\"Bounded histogram of the image (values capped at 2x the mean)\")\nplt.show()\n\n\nThreshold for the whole image: [77, 182]\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(1, 4, figsize=(15, 5))\nax[0].imshow(brain_bg, cmap=\"gray\")\nax[0].set_title(\"Background\")\nax[0].axis(\"off\")\n\nax[1].imshow(brain_gm, cmap=\"gray\")\nax[1].set_title(\"Grey matter\")\nax[1].axis(\"off\")\n\nax[2].imshow(brain_wm, cmap=\"gray\")\nax[2].set_title(\"White matter\")\nax[2].axis(\"off\")\n\nax[3].imshow(brain_bg * 1 + brain_gm * 2 + brain_wm * 3, cmap=\"gray\")\nax[3].set_title(\"All\")\nax[3].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPlot a log-scaled histogram of the image, which should show how frequently different intensity values occur in the image. How could you roughly estimate the two thresholds you found in the previous task just by looking at the histogram? (3pts)\n\n\n\n\nCode\nvalues, bin_edge = np.histogram(img, bins=256, range=(0, 256))\nbin_centers = (bin_edge[:-1] + bin_edge[1:]) / 2\nplt.bar(bin_centers, values, lw=2)\nplt.yscale(\"log\")\n\nfor th, color in zip(threshold, colors):\n    plt.axvline(th, color=color, lw=2, ls=\"--\", label=f\"Threshold: {th}\")\n\nplt.legend()\nplt.title(\"Log-scaled histogram of the image\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see, the histogram has two peaks, which correspond to the grey matter and white matter. The two thresholds can be estimated by finding the two peaks in the histogram. (The purpose of otsu thresholding is to find the optimal threshold for the two peaks)\n\n\nCombine the three masks into a single colour image so that background, grey matter, and white matter are mapped to red, green and blue, respectively. (3pts)\n\n\n\n\nCode\ncombined_brain = np.stack([brain_bg, brain_gm, brain_wm], axis=-1).astype(np.uint8) * 255\n\nplt.imshow(combined_brain)\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUse erosion (or any other morphological) filter to produce a border between the grey and white matter. Overlay that border on the denoised input image. (3pts)\n\n\n\n\nCode\nkernel = np.ones((3, 3), np.uint8)\nbrain_wm_eroded = cv2.erode(brain_wm.astype(np.uint8), kernel, iterations=1)\nbrain_wm_dilated = cv2.dilate(brain_wm_eroded, kernel, iterations=1)\nborder = (brain_wm_dilated - brain_wm_eroded) * 255\nalpha = 0.85\nbordered_img = cv2.addWeighted(img, alpha, border, 1 - alpha, 0)\n\n\n# plt.imshow(img, cmap=\"gray\")\n# plt.imshow(border, cmap=\"gray\", alpha=0.5)\nplt.imshow(bordered_img, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUse bilinear interpolation to up-sample the image by a factor of four along each axis. Apply the same thresholds as in 2) to obtain a segmentation into background, grey matter, and white matter. Up-sample the masks from 2) in the same way and compare the up-sampled masks to the masks from the up-sampled image. Can you see a diference? Why? Repeat the same procedure using nearest neighbour interpolation. Can you see a diference now? (4pts)\n\n\n\n\nCode\ndef upsample(img: np.ndarray, factor: int, interpolation: int) -&gt; np.ndarray:\n    return cv2.resize(\n        img, (img.shape[1] * factor, img.shape[0] * factor), interpolation=interpolation\n    )\n\n\nmasks, threshold = otsu_threshold(img, 3)\nimg_upsampled = upsample(img, 4, cv2.INTER_LINEAR)\nmasks_upsampled, threshold_upsampled = otsu_threshold(img_upsampled, 3)\n\nfig, ax = plt.subplots(2, 4, figsize=(15, 10))\nfig.suptitle(\n    \"Comparison of upsampled masks and upsampled image using linear interpolation\",\n    fontsize=16,\n)\n\ntitles = [\"Background\", \"Grey matter\", \"White matter\", \"All\"]\nmasks.append(masks[0] * 1 + masks[1] * 2 + masks[2] * 3)\nmasks_upsampled.append(\n    masks_upsampled[0] * 1 + masks_upsampled[1] * 2 + masks_upsampled[2] * 3\n)\n\nfor i, (mask, mask_upsampled, title) in enumerate(zip(masks, masks_upsampled, titles)):\n    ax[0, i].imshow(mask, cmap=\"gray\")\n    ax[0, i].set_title(title)\n    ax[0, i].axis(\"off\")\n\n    ax[1, i].imshow(mask_upsampled, cmap=\"gray\")\n    ax[1, i].set_title(f\"{title} upsampled\")\n    ax[1, i].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nClearly, we can see much smoother edges in the upsampled masks compared to the upsampled image. This is because the interpolation method used in the up-sampling process is linear, which smooths the edges.\nNow, let’s repeat the same procedure using the nearest neighbour interpolation method.\n\n\nCode\nmasks, threshold = otsu_threshold(img, 3)\nimg_upsampled = upsample(img, 4, cv2.INTER_NEAREST)\nmasks_upsampled, threshold_upsampled = otsu_threshold(img_upsampled, 3)\n\nfig, ax = plt.subplots(2, 4, figsize=(15, 10))\nfig.suptitle(\n    \"Comparison of upsampled masks and upsampled image using nearest neighbour interpolation\",\n    fontsize=16,\n)\n\ntitles = [\"Background\", \"Grey matter\", \"White matter\", \"All\"]\nmasks.append(masks[0] * 1 + masks[1] * 2 + masks[2] * 3)\nmasks_upsampled.append(\n    masks_upsampled[0] * 1 + masks_upsampled[1] * 2 + masks_upsampled[2] * 3\n)\n\nfor i, (mask, mask_upsampled, title) in enumerate(zip(masks, masks_upsampled, titles)):\n    ax[0, i].imshow(mask, cmap=\"gray\")\n    ax[0, i].set_title(title)\n    ax[0, i].axis(\"off\")\n\n    ax[1, i].imshow(mask_upsampled, cmap=\"gray\")\n    ax[1, i].set_title(f\"{title} upsampled\")\n    ax[1, i].axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see the edges are much sharper in the upsampled masks compared to the upsampled image. This is because the nearest neighbour interpolation method does not smooth the edges.\nTODO: Test same thing with pyrUp\n\n\nCode\ndef upsample_pyramid_(img: np.ndarray) -&gt; np.ndarray:\n    # return cv2.resize(\n    #     img, (img.shape[1] * factor, img.shape[0] * factor), interpolation=interpolation\n    # )\n    return cv2.pyrUp(img, dstsize=(img.shape[1] * 2, img.shape[0] * 2))\n\ndef upsample_pyramid(img: np.ndarray, factor: int) -&gt; np.ndarray:\n    if factor &lt;= 1:\n        raise ValueError(\"Factor should be greater than 1\")\n    f = 1\n    while f &lt; factor:\n        img = upsample_pyramid_(img)\n        f *= 2\n    return img\n\n\nprint(img.shape)\nimg_upsampled = upsample_pyramid(img, 4)\nprint(img_upsampled.shape)\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(img_upsampled, cmap=\"gray\")\nax[0].set_title(\"Upsampled image\")\nax[0].axis(\"off\")\n\nax[1].imshow(img, cmap=\"gray\")\nax[1].set_title(\"Original image\")\nax[1].axis(\"off\")\n\nplt.show()\n\n\n(145, 145)\n(580, 580)"
  },
  {
    "objectID": "reports/task2.html",
    "href": "reports/task2.html",
    "title": "task 2",
    "section": "",
    "text": "Task 2\nFor the subjective enhancement of a greyscale image \\(G = g(x, y)\\) , a transformation \\(T_G\\) is performed as a so-called gamma correction in the form \\(T_G : g \\rightarrow f\\) with \\(f(x, y) = c g^\\gamma (x, y)\\) where \\(g, f \\in [0, 255]\\).\n\nSketch the transformation curve \\(T_G\\) for \\(\\gamma_1 = 0.5\\) and \\(\\gamma_2 = 2\\)\n\n\nThe first step is to find the values of \\(c\\) for both cases. Since \\(\\max(f) = \\max(g) = 255\\), we have \\(c = 255 / 255^\\gamma\\).\n\n\nCode\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ndef draw_transform_curve(gamma: float, ax: plt.Axes = None, label: bool = True):\n    if not ax:\n        fig, ax = plt.subplots()\n    x = np.linspace(0, 255, 256)\n    c = 255 / 255**gamma\n    y = c * x**gamma\n    message = f\"$f = {c:0.4f} \\\\times g^{{{gamma}}}$\"\n    if label:\n        ax.plot(x, y, label=message)\n    else:\n        ax.plot(x, y)\n    ax.set_xlabel(\"g\")\n    if label:\n        ax.set_ylabel(\"f\")\n    else:\n        ax.set_ylabel(message)\n\n\nfig, ax = plt.subplots()\nfor gamma in [0.5, 2]:\n    draw_transform_curve(gamma, ax)\nax.set_title(f\"Transformation curve for $\\\\gamma=0.5$ and $\\\\gamma=2$\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nHow is the coeficient c typically determined? (2pts)\n\n\nThe coefficient \\(c\\) is typically determined such that the maximum value of the input image is mapped to the maximum value of the output image. This is done to ensure that the full dynamic range of the output image is used.\nAs mentioned above, \\(c = 255 / 255^\\gamma\\).\n\n\nIn which respect and for which type of input images \\(G\\) do the two gamma values \\(\\gamma_1,\\;\\gamma_2\\) lead to an image enhancement respectively? (2pts)\n\n\nFor \\(\\gamma &lt; 1\\), the transformation curve is concave, which means that the lower intensity values are stretched more than the higher intensity values. This leads to a brighter image with more contrast. This is useful for images with low contrast.\nFor \\(\\gamma &gt; 1\\), the transformation curve is convex, which means that the higher intensity values are stretched more than the lower intensity values. This leads to a darker image with more contrast. This is useful for images with high contrast.\n\n\nWhat should be the minimum slope of the transform function?\n\nfor a grey value spread (2pts)\nfor a grey value compression (2pts)\n\n\n\nIt’s important to note that a slope of exactly 1 implies no change in contrast, as the transformation function becomes an identity mapping. Also, a slope of 0 implies that the output image will be a constant value, which is not useful for image enhancement.\n\nFor a grey value spread, the minimum slope of the transform function should be 1.\nFor a grey value compression, the minimum slope of the transform function should be 0 (and smaller than 1). For instance, in this function:\n\n\n\n\n\n\n\n\n\n\nAs we can see, the gray values between (80, 160) are streched between (40, 200) which has a slope greater than 1. On the other hand, the gray values between (160, 255) are compressed between (200, 255) which has a slope smaller than 1."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "imaging_ai",
    "section": "",
    "text": "Code\nimport sys\nsys.executable\n\n\n'/opt/hostedtoolcache/Python/3.10.16/x64/bin/python3'\ntitle: Task 1"
  },
  {
    "objectID": "index.html#data-handling-and-preprocessing-10-points",
    "href": "index.html#data-handling-and-preprocessing-10-points",
    "title": "imaging_ai",
    "section": "Data Handling and Preprocessing (10 Points)",
    "text": "Data Handling and Preprocessing (10 Points)\n\n\nYou can focus for now on loading the T1-weighted images and the matching labels.\nCreate a dataloader for the data using PyTorch’s Dataloader (or Monai’s Dataloader class)\nCreate suitable augmentations for the task to solve. Please note: If you apply transformations to the input data, you should think about if you need to apply any transformation to the label of the image as well.\n\n\n\nWe didnt used Monai’s Dataloader class, because we wanted to use the data in the HPC. The dataset implementation is availabel in the task4/brats_segmentations/dataloader.py\n\n\n\nfile name: task4/brats_segmentation/dataloader.py\n\n\nclass BraTSDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset for BraTS data.\n    \"\"\"\n\n    def __init__(self, base_path: str | os.PathLike, transform=None, limit=None):\n        \"\"\"\n        Args:\n            base_path (str): Path to the dataset directory.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n\n        all_patients = list(glob.glob(os.path.join(base_path, \"BraTS*\")))\n        if not all_patients:\n            raise ValueError(f\"No patients found in {base_path}\")\n        else:\n            print(f\"Found {len(all_patients)} patients in {base_path}\")\n        if limit:\n            self.patient_dirs = all_patients[:limit]  # Limit to 20 patients for now\n        self.patient_dirs = all_patients\n\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.patient_dirs)\n\n    def __getitem__(\n        self, idx\n    ) -&gt; dict[Literal[\"t1\", \"t1ce\", \"t2\", \"flair\", \"label\"], np.ndarray]:\n        \"\"\"\n        Load data and labels for a given patient.\n        \"\"\"\n        patient_dir = self.patient_dirs[idx]\n        sample = {}\n\n        # Load the four MRI modalities\n        modalities = [\"t1\", \"t1ce\", \"t2\", \"flair\"]\n        modalities = [\"t1\"]\n        print(patient_dir)\n        for modality in modalities:\n            file_path = os.path.join(\n                patient_dir, f\"{os.path.basename(patient_dir)}_{modality}.nii.gz\"\n            )\n            # sample[modality] = nib.load(file_path).get_fdata()\n            sample[modality] = file_path\n\n        # Load the segmentation label\n        label_path = os.path.join(\n            patient_dir, f\"{os.path.basename(patient_dir)}_seg.nii.gz\"\n        )\n        # sample[\"label\"] = nib.load(label_path).get_fdata()\n        sample[\"label\"] = label_path\n\n        if self.transform:\n            # for modality in modalities:\n            #     sample[modality] = self.transform(sample[modality])\n            sample_ = {}\n            sample_[\"image\"] = sample[\"t1\"]\n            sample_[\"label\"] = sample[\"label\"]\n            print(sample_)\n            sample = self.transform(sample_)\n\n        return sample"
  },
  {
    "objectID": "reports/task1.html",
    "href": "reports/task1.html",
    "title": "Task 1",
    "section": "",
    "text": "Task 1\nThe Fourier transformation \\(f(x, y) \\rightarrowtail F(u, v)\\) of a greyscale image \\(f(x,y)\\) results in a band-limited signal in the spatial frequency range with maximum frequencies \\(f_{umax}\\) and \\(ƒ_{vmax}\\). For representation in the computer, the (partial) image is sampled in x direction with 20 sampling points per mm and in y direction with 10 sampling points per mm.\n\n\nWhat is the theoretical maximum value of \\(f_{umax}\\) and \\(f_{vmax}\\) if error-free image reconstruction from the digital image should be possible (not using any compressive-sensing techniques)? (6pts)\n\n\n\nAccording to the Nyquist sampling theorem, the maximum representable frequency (Nyquist frequency) in each direction is half the sampling frequency. The sampling frequency can be derived from the given sampling points per mm.\n\nSampling frequency in x is \\(f_{sx}\\) and the Nyquist frequency in x is \\(f_{umax}\\): \\[\\begin{align*}\n  &f_{sx} = 20\\;\\text{points/mm} = 20 \\times 10^3 \\, \\text{points/m} \\\\\n  \\implies &f_{umax} = \\frac{f_{sx}}{2} = 10.0 \\, \\text{kHz}\n\\end{align*}\\]\nSampling frequency in y is \\(f_{sy}\\) and the Nyquist frequency in y is \\(f_{vmax}\\): \\[\\begin{align*}\n  &f_{sy} = 10 \\, \\text{points/mm} = 10 \\times 10^3 \\, \\text{points/m} \\\\\n  \\implies &f_{vmax} = \\frac{f_{sy}}{2} = 5.0 \\, \\text{kHz}\n\\end{align*}\\]\n\nThis ensures error-free reconstruction, as the digital image will contain all frequency components of the original image within the Nyquist limit. Frequencies above these limits would result in aliasing, violating error-free reconstruction conditions.\n\n\nWhat is the minimum memory requirement for the color image \\(f_F(x, y)\\) when stored in a conventional computer system, if \\(1024\\) values are to be distinguished per color channel. Describe the image format to be used.\n\n\nTo start lets find the number of ixels\nLet the image dimensions in mm be \\(L_x\\) (width) and \\(L_y\\) (height).\n- Pixels in \\(x\\)-direction: \\(N_x = 10.0 \\cdot L_x\\) - Pixels in \\(y\\)-direction: \\(N_y = 5.0 \\cdot L_y\\) - Total number of pixels: \\[\nN_{\\text{pixels}} = N_x \\cdot N_y = 50.0 \\cdot L_x \\cdot L_y\n\\]\nEach pixel in a color image has values for three color channels: Red, Green, and Blue (RGB). Each channel can store \\(1024\\) distinct values, which means \\(log_2^{1024} = 10.0\\) bits per channel.\nTotal bits per pixel: \\(b = 10.0 \\times 3 = 30.0\\) bits/pixel.\nThe memory requirement is the product of the number of pixels and bits per pixel: \\[\n\\text{Used Memory} = N_{\\text{pixels}} \\cdot b = (50.0 \\cdot L_x \\cdot L_y) \\cdot b \\, \\text{bits} = 6.25 \\cdot L_x \\cdot L_y \\cdot 30.0 \\, \\text{bytes}\n\\]\nTODO: Im not sure about the answer, I think it should not depend on \\(L_x\\) and \\(L_y\\).\n\n\nHow many colors could be represented with the quantization chosen in sub-task 3? (2pts)\n\n\nEach channel (Red, Green, and Blue) can represent 1024 intensity levels. With 10 bits per channel and 3 channels, the total number of colors is: [ = 1024^3 = 1,073,741,824 ]"
  },
  {
    "objectID": "reports/task4.html",
    "href": "reports/task4.html",
    "title": "Task 4",
    "section": "",
    "text": "You can focus for now on loading the T1-weighted images and the matching labels.\nCreate a dataloader for the data using PyTorch’s Dataloader (or Monai’s Dataloader class)\nCreate suitable augmentations for the task to solve. Please note: If you apply transformations to the input data, you should think about if you need to apply any transformation to the label of the image as well.\n\n\n\nWe didnt used Monai’s Dataloader class, because we wanted to use the data in the HPC. The dataset implementation is availabel in the task4/brats_segmentations/dataloader.py\n\n\n\nfile name: ../task4/brats_segmentation/dataloader.py\n\n\nclass BraTSDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset for BraTS data.\n    \"\"\"\n\n    def __init__(self, base_path: str | os.PathLike, transform=None, limit=None):\n        \"\"\"\n        Args:\n            base_path (str): Path to the dataset directory.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n\n        all_patients = list(glob.glob(os.path.join(base_path, \"BraTS*\")))\n        if not all_patients:\n            raise ValueError(f\"No patients found in {base_path}\")\n        else:\n            print(f\"Found {len(all_patients)} patients in {base_path}\")\n        if limit:\n            self.patient_dirs = all_patients[:limit]  # Limit to 20 patients for now\n        self.patient_dirs = all_patients\n\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.patient_dirs)\n\n    def __getitem__(\n        self, idx\n    ) -&gt; dict[Literal[\"t1\", \"t1ce\", \"t2\", \"flair\", \"label\"], np.ndarray]:\n        \"\"\"\n        Load data and labels for a given patient.\n        \"\"\"\n        patient_dir = self.patient_dirs[idx]\n        sample = {}\n\n        # Load the four MRI modalities\n        modalities = [\"t1\", \"t1ce\", \"t2\", \"flair\"]\n        modalities = [\"t1\"]\n        print(patient_dir)\n        for modality in modalities:\n            file_path = os.path.join(\n                patient_dir, f\"{os.path.basename(patient_dir)}_{modality}.nii.gz\"\n            )\n            # sample[modality] = nib.load(file_path).get_fdata()\n            sample[modality] = file_path\n\n        # Load the segmentation label\n        label_path = os.path.join(\n            patient_dir, f\"{os.path.basename(patient_dir)}_seg.nii.gz\"\n        )\n        # sample[\"label\"] = nib.load(label_path).get_fdata()\n        sample[\"label\"] = label_path\n\n        if self.transform:\n            # for modality in modalities:\n            #     sample[modality] = self.transform(sample[modality])\n            sample_ = {}\n            sample_[\"image\"] = sample[\"t1\"]\n            sample_[\"label\"] = sample[\"label\"]\n            print(sample_)\n            sample = self.transform(sample_)\n\n        return sample"
  },
  {
    "objectID": "reports/task4.html#data-handling-and-preprocessing-10-points",
    "href": "reports/task4.html#data-handling-and-preprocessing-10-points",
    "title": "Task 4",
    "section": "",
    "text": "You can focus for now on loading the T1-weighted images and the matching labels.\nCreate a dataloader for the data using PyTorch’s Dataloader (or Monai’s Dataloader class)\nCreate suitable augmentations for the task to solve. Please note: If you apply transformations to the input data, you should think about if you need to apply any transformation to the label of the image as well.\n\n\n\nWe didnt used Monai’s Dataloader class, because we wanted to use the data in the HPC. The dataset implementation is availabel in the task4/brats_segmentations/dataloader.py\n\n\n\nfile name: ../task4/brats_segmentation/dataloader.py\n\n\nclass BraTSDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset for BraTS data.\n    \"\"\"\n\n    def __init__(self, base_path: str | os.PathLike, transform=None, limit=None):\n        \"\"\"\n        Args:\n            base_path (str): Path to the dataset directory.\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n\n        all_patients = list(glob.glob(os.path.join(base_path, \"BraTS*\")))\n        if not all_patients:\n            raise ValueError(f\"No patients found in {base_path}\")\n        else:\n            print(f\"Found {len(all_patients)} patients in {base_path}\")\n        if limit:\n            self.patient_dirs = all_patients[:limit]  # Limit to 20 patients for now\n        self.patient_dirs = all_patients\n\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.patient_dirs)\n\n    def __getitem__(\n        self, idx\n    ) -&gt; dict[Literal[\"t1\", \"t1ce\", \"t2\", \"flair\", \"label\"], np.ndarray]:\n        \"\"\"\n        Load data and labels for a given patient.\n        \"\"\"\n        patient_dir = self.patient_dirs[idx]\n        sample = {}\n\n        # Load the four MRI modalities\n        modalities = [\"t1\", \"t1ce\", \"t2\", \"flair\"]\n        modalities = [\"t1\"]\n        print(patient_dir)\n        for modality in modalities:\n            file_path = os.path.join(\n                patient_dir, f\"{os.path.basename(patient_dir)}_{modality}.nii.gz\"\n            )\n            # sample[modality] = nib.load(file_path).get_fdata()\n            sample[modality] = file_path\n\n        # Load the segmentation label\n        label_path = os.path.join(\n            patient_dir, f\"{os.path.basename(patient_dir)}_seg.nii.gz\"\n        )\n        # sample[\"label\"] = nib.load(label_path).get_fdata()\n        sample[\"label\"] = label_path\n\n        if self.transform:\n            # for modality in modalities:\n            #     sample[modality] = self.transform(sample[modality])\n            sample_ = {}\n            sample_[\"image\"] = sample[\"t1\"]\n            sample_[\"label\"] = sample[\"label\"]\n            print(sample_)\n            sample = self.transform(sample_)\n\n        return sample"
  }
]